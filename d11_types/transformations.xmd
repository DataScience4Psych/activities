---

middle: Transformations


---

## Log transformation

+ In an intro course, you probably learned to use the most common transformation, the log.

+ The main reason we gave was that it often made positive data more normal.

+ But there's another and perhaps more fundamental reason: 

+ It often leads to differences between samples that we can interpret as a *multiplicative shift*.

+ Some statisticians will go as far as to recommend log transforming positive data by default...

<!--, though by the end of Cleveland's chapter 2, we'll see an example where that backfires.
-->
---

## Power transformations

+ The log transformation doesn't always work -- 
	+ for a start, you can't log zero or a negative number.

+ **Power transformations** allow a wider range of options.

+ Define a power transformation with parameter $\tau$ to be $x^\tau$.

+ Where practical, we'll hugely prefer $\tau = 1$ (no transformation), $\tau = 0$, or possibly $\tau = -1$ (inverse transformation) because they're much easier to interpret.

---

## Box-Cox transformation

+ You might also see Box-Cox transformations: 
	+ these are really just power transformations, 
	+ but re-scaled so that they go continuously to the log transform as $\tau \to 0$ on either side.

+ Let $x$ be the original data value, 
+ and let $f_\tau$ be the Box-Cox transformation with parameter $\tau$. 
+ It is defined as:

$$
f_\tau(x) = \begin{cases}
\frac{x^{\tau} - 1}{\tau} & \tau \ne 0\\
\log x & \tau = 0
\end{cases}
$$
---


Here's a little picture of the transformations, note how the log transformation floats nicely in between the negative and positive values of $\tau$.

```{r}
x = seq(0, 5, length.out = 100)[-1]
bc_trans = function(x, tau) {
    if(tau == 0){
        return(log(x))
    }
    return((x^tau - 1) / tau)
}
tau_vec = seq(-.4, 1.4, by = .2)
transforms = sapply(tau_vec, function(tau) bc_trans(x, tau))
transforms_melted = melt(transforms)
transforms_for_plotting = transforms_melted %>% mutate(x = x[Var1], tau = tau_vec[Var2])
ggplot(transforms_for_plotting) +
    geom_line(aes(x = x, y = value, color = as.factor(tau))) +
    xlab("Original Data Value") + ylab("Transformed Value")
```


## Let's see how these work on real data

Load `ggplot2`:

```{r}
library(ggplot2)
```

We'll also use an R workspace prepared by Cleveland to use in conjunction with his book. Among other things, this .RData contains the data sets used in the book.

```{r}
load("data/lattice.RData")
```



We'll use the stereogram fusion time data in `fusion.time`, which contains the group variable `nv.vv` (where "NV" means no visual information and "VV" means visual information) and the quantitative variable `time` (a positive number.)


## Power transformations on fusion time data

Let's try some of the power transformations on the fusion time data --- we would like to see which gives a distribution closest to normal. (Why?)

We'll just look at the VV times for now.

```{r, fig.width = 10, fig.height = 4, dpi = 150}
vv = fusion.time %>% subset(nv.vv == "VV")
tau_vec = seq(-1, 1, by = .25)
transforms = sapply(tau_vec, function(tau) bc_trans(vv$time, tau))
transforms_melted = melt(transforms)
transforms_for_plotting = transforms_melted %>% mutate(tau = tau_vec[Var2])

ggplot(transforms_for_plotting) +
    stat_qq(aes(sample = value)) +
    facet_wrap(~ tau, scales = "free", ncol = 5)
```

Here $\tau$-values of $0$ (the log transformation) and $-0.25$ give the straightest normal QQ plots.

Since it's much, much easier to interpret $\log(x)$ than $x^{-0.25}$, we strongly prefer the log transformation.

## Theoretical reasons for transformations

In your more theoretical statistics courses, you might come across _variance-stabilizing transformations_.

In real data, we often see that the variance depends on the mean.

. . .

For example, if $X$ is distributed $\text{Poisson}(\lambda)$, $E_\lambda(X) = \lambda$, and $\text{Var}_\lambda(X) = \lambda$.

For Poisson random variables, the square root transformation ($\tau =
1/2$) is approximately variance stabilizing, that is, $X^{1/2}$ will
have variance that is about the same no matter what $\lambda$ is.

Conut data often have a Poisson distribution, and so it is often
useful to use a square root transformation for counts.

> The re-expressions most frequently useful for counts are logs and (square) roots. In fact, it is rather hard to find a set of counts that are better analyzed as raw counts than as root counts.

Tukey, EDA, p. 83-84

. . .

If we have random variables $X_i$, with $E(X_i) = \mu$ and $\text{Var}(X_i) = s^2 \mu^2$ (the standard deviation is proportional to the mean), then the log transformation ($\tau = 0$) is approximately variance stabilizing.

. . .


---